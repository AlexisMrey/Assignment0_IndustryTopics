{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 0\n",
    "\n",
    "**Name:** Alvaro Alexis Muñoz Reynoso\n",
    "\n",
    "**e-mail:** alvaro.munoz7503@alumnos.udg.mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory on the Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booth Function\n",
    "\n",
    "- Given the most common functions to test Gradient Descent algorithm, i've selected the Booth Function.\n",
    "\n",
    "- On next code we will define the booth function and its corresponding gradient returning an array with `x` and `y` derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining booth function\n",
    "def booth_function(x, y):\n",
    "    return (x + 2*y - 7)**2 + (2*x + y - 5)**2\n",
    "\n",
    "# Defining booth function gradient\n",
    "def booth_gradient(x, y):\n",
    "    df_dx = 2*(x + 2*y - 7) + 4*(2*x + y - 5)\n",
    "    df_dy = 4*(x + 2*y - 7) + 2*(2*x + y - 5)\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Function Algorithm\n",
    "\n",
    "At a first glance we can observe three parameters that the function receives, here is a brief explanation of each one:\n",
    "\n",
    " - **learning rate**: It's the constant that defines how large will be the \"steps taken\" on each iteration until reaching the local minimun point.\n",
    " - **max_iterations**: It's the loop limit in which the algorithm will be repeated to \"give a step\" on each iteration to get \"closer and closer\" to the function local minimum point.\n",
    " - **tolerance**: It is the convergence criteria used to know when to stop the loop, in other words if the distance \"walked\" between each \"step taken\" of the actual and the previous iteration is smaller than the defined tolerance, it will be infered that the minimun function point has been reached.\n",
    "\n",
    "In each iteration the `x` and `y` values ​​are stored in an array, so that the path taken to find the minimum point of the function can later be plotted. Then when the loop starts, `x` and `y` values are evaluated by the gradient(derivative) of Booth function, after that the new values \"steps\" to be taken are calculated with the `gradient descendent algorithm` that is the current x/y values minus the constant designated at the beginning (learning rate) multiplied by the correspondient gradients/derivatives for `x` and `y`.\n",
    "The difference/distance between the new point and the one of previous iteration is measured with the `Euclidean distance` method, that consists on a square root where current and old values of `x` and `y` are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate=0.01, max_iterations=100, tolerance=1e-6):\n",
    "    #x, y = np.random.uniform(-10, 10, 2)  # Random initialization\n",
    "    x, y = (8,8)\n",
    "    path = [(x, y)]  # To save the path taken\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        grad = booth_gradient(x, y)\n",
    "        x_new = x - learning_rate * grad[0]\n",
    "        y_new = y - learning_rate * grad[1]\n",
    "        path.append((x_new, y_new))\n",
    "\n",
    "        # If the difference is smaller than tolerance the loop ends \n",
    "        if np.sqrt((x_new - x)**2 + (y_new - y)**2) < tolerance:\n",
    "            break\n",
    "\n",
    "        x, y = x_new, y_new\n",
    "\n",
    "    return np.array(path), booth_function(x, y)\n",
    "\n",
    "# Execute the algorithm\n",
    "path, final_value = gradient_descent(learning_rate=0.01)\n",
    "\n",
    "print(f\"Minimum value reached: {final_value}\")\n",
    "print(f\"Minimun point: {path[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
